spring.data.elasticsearch.cluster-name=elasticsearch
#spring.data.elasticsearch.cluster-nodes=47.96.87.131:9300
spring.data.elasticsearch.cluster-nodes=118.89.142.11:9300

server.port=9999


##数据库连接信息
spring.datasource.username=cdb_outerroot
spring.datasource.password=v47hDkRUHYIY
spring.datasource.url=jdbc:mysql://5950b916141b6.sh.cdb.myqcloud.com:16454/zlwon-v3
#spring.datasource.url=jdbc:mysql://5950b916141b6.sh.cdb.myqcloud.com:16454/zlwon-v3-test
spring.datasource.driver-class-name=com.mysql.jdbc.Driver
###################以下为druid增加的配置###########################
spring.datasource.type=com.alibaba.druid.pool.DruidDataSource
# 下面为连接池的补充设置，应用到上面所有数据源中
# 初始化大小，最小，最大
spring.datasource.initialSize=5
spring.datasource.minIdle=5
spring.datasource.maxActive=20
# 配置获取连接等待超时的时间
spring.datasource.maxWait=60000
# 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒
spring.datasource.timeBetweenEvictionRunsMillis=60000
# 配置一个连接在池中最小生存的时间，单位是毫秒
spring.datasource.minEvictableIdleTimeMillis=300000
spring.datasource.validationQuery=SELECT 1 FROM DUAL
spring.datasource.testWhileIdle=true
spring.datasource.testOnBorrow=false
spring.datasource.testOnReturn=false
# 打开PSCache，并且指定每个连接上PSCache的大小
spring.datasource.poolPreparedStatements=true
spring.datasource.maxPoolPreparedStatementPerConnectionSize=20
# 配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙，不知道为何这个非要加druid
spring.datasource.druid.filters=stat,wall,log4j
# 通过connectProperties属性来打开mergeSql功能；慢SQL记录
spring.datasource.connectionProperties=druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000
# 合并多个DruidDataSource的监控数据
#spring.datasource.useGlobalDataSourceStat=true
###############以上为配置druid添加的配置########################################


###########kafka消费者配置##############
#指定kafka代理地址(zookeeper)，可以多个逗号隔开
spring.kafka.consumer.bootstrap-servers=47.96.87.131:9092
#指定默认消费者组id
spring.kafka.consumer.group-id=zlwon
#自动提交
spring.kafka.consumer.enable-auto-commit=true
# earliest:当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
# latest:当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据
# none:topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
spring.kafka.consumer.auto-offset-reset=latest



#添加或更新物性主题
kafka.topic.add.specification=addSpecification
#添加或更新案例主题
kafka.topic.add.applicationCase=addApplicationCase
#新增提问主题
kafka.topic.add.questions=addQuestions
#新增物性标签
kafka.topic.add.characteristic=addCharacteristic
#新增物性报价
kafka.topic.add.dealerdQuotation=addDealerdQuotation
#新增或更新用户主题
kafka.topic.add.customer=addCustomer

mybatis.mapper-locations=classpath:com/zlwon/mapper/impl/*.xml